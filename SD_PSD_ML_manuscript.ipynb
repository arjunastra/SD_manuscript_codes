{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DBS003', 'DBS005', 'DBS013', 'DBS014', 'DBS018', 'DBS021',\n",
       "       'DBS036', 'DBS047', 'DBS050', 'DBS039'], dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SD_PSD_ML_manuscript.ipynb\n",
    "#Arjun Balachandar Oct 2024\n",
    "#This code takes the PSDs for each recording, and labels them as either SD or non-SD (often referred to as oneYear in the code), and then trains a machine learning model\n",
    "# to predict the label of a new PSD, using leave-one-out cross validation.\n",
    "\n",
    "#Directories and File Names\n",
    "PATH = #change directory path accordingly\n",
    "lfpDir = PATH + 'data_processed/'\n",
    "clinVarFile = PATH + 'statusDystonicusDates_NEW.csv'\n",
    "implantDateFile = PATH + 'patientSummary.csv'\n",
    "optimalChannelsFile = PATH + 'AvailabilitySummary - with optimal channels - deidentified.xlsx' # Determined in consultation with Dr George Ibrahim\n",
    "resultsDir = PATH + 'Analysis/'\n",
    "resultsLFPPowersDir = resultsDir + 'LFP Powers/'\n",
    "resultsPSDsDir = resultsDir + 'PSDs/'\n",
    "resultsPSDPeaksDir = resultsDir + 'PSDPeaks/'\n",
    "resultsFilename = 'powerChangesAfterCuringStatusDystonicus.csv'\n",
    "resultsPSDFilename = 'PSDsAfterCuringStatusDystonicus.csv'\n",
    "resultsCohDir = resultsDir + 'Coherence/'\n",
    "resultsCohFilename = 'coherenceAfterCuringStatusDystonicus.csv'\n",
    "demographicsDir = PATH + 'Demographics/'\n",
    "demographicsFile = demographicsDir + 'MulticentrePediatric-Demographics and clinical data (raw)_ArjunEdit.csv'\n",
    "moredemographicsDir = PATH + 'Demographics/Additional Metrics/' #additional clinical metrics\n",
    "moredemographicsBasicInfo = moredemographicsDir + 'MulticentrePediatric-ClinicalData Dystonia Labelled.csv'\n",
    "moredemographicsFile_baseline = moredemographicsDir + 'MulticentrePediatric-BaselineDystonia.csv'\n",
    "moredemographicsFile_6m = moredemographicsDir + 'MulticentrePediatric-6mDystonia.csv'\n",
    "moredemographicsFile_1y = moredemographicsDir + 'MulticentrePediatric-1yDystonia.csv'\n",
    "moredemographicsFile_2y = moredemographicsDir + 'MulticentrePediatric-2yDystonia.csv'\n",
    "moredemographicsFile_3y = moredemographicsDir + 'MulticentrePediatric-3yDystonia.csv'\n",
    "\n",
    "#Frequency bands\n",
    "betaLim = [12.5, 30] # In Hertz, define the beta band\n",
    "thetaLim = [3, 7] # 3.5-7 as per Mark, Was [4,7] before 31 Jan\n",
    "deltaLim = [1,3]\n",
    "alphaLim = [7,12.5]\n",
    "gammaLim = [30,60]\n",
    "lowbetaLim = [12.5, 20]\n",
    "highbetaLim = [20, 30]\n",
    "lowgammaLim = [30, 40]\n",
    "highgammaLim = [40, 60]\n",
    "channelOverride = False # True # Should be false. Set to true if you want to set the channel manually (not recommended).\n",
    "z_score_norm = False #Normalize the LFP values by z-scoring normalization if True\n",
    "normalize_PSD = True #Normalize the PSD values by dividing by sum of total power of PSD\n",
    "\n",
    "# Initializations\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import signal\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore\n",
    "import re #regular expression\n",
    "import neurodsp\n",
    "from neurodsp.utils.norm import normalize_sig\n",
    "from neurodsp.utils import create_times\n",
    "from neurodsp.plts.time_series import plot_time_series, plot_instantaneous_measure\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, FunctionTransformer\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.norms import HuberT\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import kruskal\n",
    "import statsmodels.formula.api as smf\n",
    "import scikit_posthocs as sp\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "\n",
    "#Machine Learning\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn_lvq import GrmlvqModel\n",
    "from sklvq import LGMLVQ, GMLVQ\n",
    "from sklearn_lvq.utils import plot2d\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "diff = lambda x, y: y - x\n",
    "\n",
    "# Load and process the dates\n",
    "clinVars_df = pd.read_csv(clinVarFile, delimiter=',')\n",
    "implantDates_df = pd.read_csv(implantDateFile, delimiter=',')\n",
    "patList = clinVars_df['Subject ID'].values\n",
    "patList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:80: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:80: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/qr/d2wxfs456fsf5lgfshrqbsmw0000gn/T/ipykernel_10585/224699477.py:80: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "Training: 1 of 184\n",
      "182 1\n",
      "Training: 2 of 184\n",
      "182 1\n",
      "Training: 3 of 184\n",
      "182 1\n",
      "Training: 4 of 184\n",
      "182 1\n",
      "Training: 5 of 184\n",
      "182 1\n",
      "Training: 6 of 184\n",
      "182 1\n",
      "Training: 7 of 184\n",
      "182 1\n",
      "Training: 8 of 184\n",
      "182 1\n",
      "Training: 9 of 184\n",
      "182 1\n",
      "Training: 10 of 184\n",
      "182 1\n",
      "Training: 11 of 184\n",
      "182 1\n",
      "Training: 12 of 184\n",
      "182 1\n",
      "Training: 13 of 184\n",
      "182 1\n",
      "Training: 14 of 184\n",
      "182 1\n",
      "Training: 15 of 184\n",
      "182 1\n",
      "Training: 16 of 184\n",
      "182 1\n",
      "Training: 17 of 184\n",
      "182 1\n",
      "Training: 18 of 184\n",
      "182 1\n",
      "Training: 19 of 184\n",
      "182 1\n",
      "Training: 20 of 184\n",
      "182 1\n",
      "Training: 21 of 184\n",
      "182 1\n",
      "Training: 22 of 184\n",
      "182 1\n",
      "Training: 23 of 184\n",
      "182 1\n",
      "Training: 24 of 184\n",
      "182 1\n",
      "Training: 25 of 184\n",
      "182 1\n",
      "Training: 26 of 184\n",
      "182 1\n",
      "Training: 27 of 184\n",
      "182 1\n",
      "Training: 28 of 184\n",
      "182 1\n",
      "Training: 29 of 184\n",
      "182 1\n",
      "Training: 30 of 184\n",
      "182 1\n",
      "Training: 31 of 184\n",
      "182 1\n",
      "Training: 32 of 184\n",
      "182 1\n",
      "Training: 33 of 184\n",
      "182 1\n",
      "Training: 34 of 184\n",
      "182 1\n",
      "Training: 35 of 184\n",
      "182 1\n",
      "Training: 36 of 184\n",
      "182 1\n",
      "Training: 37 of 184\n",
      "182 1\n",
      "Training: 38 of 184\n",
      "182 1\n",
      "Training: 39 of 184\n",
      "182 1\n",
      "Training: 40 of 184\n",
      "182 1\n",
      "Training: 41 of 184\n",
      "182 1\n",
      "Training: 42 of 184\n",
      "182 1\n",
      "Training: 43 of 184\n",
      "182 1\n",
      "Training: 44 of 184\n",
      "182 1\n",
      "Training: 45 of 184\n",
      "182 1\n",
      "Training: 46 of 184\n",
      "182 1\n",
      "Training: 47 of 184\n",
      "182 1\n",
      "Training: 48 of 184\n",
      "182 1\n",
      "Training: 49 of 184\n",
      "182 1\n",
      "Training: 50 of 184\n",
      "182 1\n",
      "Training: 51 of 184\n",
      "182 1\n",
      "Training: 52 of 184\n",
      "182 1\n",
      "Training: 53 of 184\n",
      "182 1\n",
      "Training: 54 of 184\n",
      "182 1\n",
      "Training: 55 of 184\n",
      "182 1\n",
      "Training: 56 of 184\n",
      "182 1\n",
      "Training: 57 of 184\n",
      "182 1\n",
      "Training: 58 of 184\n",
      "182 1\n",
      "Training: 59 of 184\n",
      "182 1\n",
      "Training: 60 of 184\n",
      "182 1\n",
      "Training: 61 of 184\n",
      "182 1\n",
      "Training: 62 of 184\n",
      "182 1\n",
      "Training: 63 of 184\n",
      "182 1\n",
      "Training: 64 of 184\n",
      "182 1\n",
      "Training: 65 of 184\n",
      "182 1\n",
      "Training: 66 of 184\n",
      "182 1\n",
      "Training: 67 of 184\n",
      "182 1\n",
      "Training: 68 of 184\n",
      "182 1\n",
      "Training: 69 of 184\n",
      "182 1\n",
      "Training: 70 of 184\n",
      "182 1\n",
      "Training: 71 of 184\n",
      "182 1\n",
      "Training: 72 of 184\n",
      "182 1\n",
      "Training: 73 of 184\n",
      "182 1\n",
      "Training: 74 of 184\n",
      "182 1\n",
      "Training: 75 of 184\n",
      "182 1\n",
      "Training: 76 of 184\n",
      "182 1\n",
      "Training: 77 of 184\n",
      "182 1\n",
      "Training: 78 of 184\n",
      "182 1\n",
      "Training: 79 of 184\n",
      "182 1\n",
      "Training: 80 of 184\n",
      "182 1\n",
      "Training: 81 of 184\n",
      "182 1\n",
      "Training: 82 of 184\n",
      "182 1\n",
      "Training: 83 of 184\n",
      "182 1\n",
      "Training: 84 of 184\n",
      "182 1\n",
      "Training: 85 of 184\n",
      "182 1\n",
      "Training: 86 of 184\n",
      "182 1\n",
      "Training: 87 of 184\n",
      "182 1\n",
      "Training: 88 of 184\n",
      "182 1\n",
      "Training: 89 of 184\n",
      "182 1\n",
      "Training: 90 of 184\n",
      "182 1\n",
      "Training: 91 of 184\n",
      "182 1\n",
      "Training: 92 of 184\n",
      "182 1\n",
      "Training: 93 of 184\n",
      "182 1\n",
      "Training: 94 of 184\n",
      "182 1\n",
      "Training: 95 of 184\n",
      "182 1\n",
      "Training: 96 of 184\n",
      "182 1\n",
      "Training: 97 of 184\n",
      "182 1\n",
      "Training: 98 of 184\n",
      "182 1\n",
      "Training: 99 of 184\n",
      "182 1\n",
      "Training: 100 of 184\n",
      "182 1\n",
      "Training: 101 of 184\n",
      "182 1\n",
      "Training: 102 of 184\n",
      "182 1\n",
      "Training: 103 of 184\n",
      "182 1\n",
      "Training: 104 of 184\n",
      "182 1\n",
      "Training: 105 of 184\n",
      "182 1\n",
      "Training: 106 of 184\n",
      "182 1\n",
      "Training: 107 of 184\n",
      "182 1\n",
      "Training: 108 of 184\n",
      "182 1\n",
      "Training: 109 of 184\n",
      "182 1\n",
      "Training: 110 of 184\n",
      "182 1\n",
      "Training: 111 of 184\n",
      "182 1\n",
      "Training: 112 of 184\n",
      "182 1\n",
      "Training: 113 of 184\n",
      "182 1\n",
      "Training: 114 of 184\n",
      "182 1\n",
      "Training: 115 of 184\n",
      "182 1\n",
      "Training: 116 of 184\n",
      "182 1\n",
      "Training: 117 of 184\n",
      "182 1\n",
      "Training: 118 of 184\n",
      "182 1\n",
      "Training: 119 of 184\n",
      "182 1\n",
      "Training: 120 of 184\n",
      "182 1\n",
      "Training: 121 of 184\n",
      "182 1\n",
      "Training: 122 of 184\n",
      "182 1\n",
      "Training: 123 of 184\n",
      "182 1\n",
      "Training: 124 of 184\n",
      "182 1\n",
      "Training: 125 of 184\n",
      "182 1\n",
      "Training: 126 of 184\n",
      "182 1\n",
      "Training: 127 of 184\n",
      "182 1\n",
      "Training: 128 of 184\n",
      "182 1\n",
      "Training: 129 of 184\n",
      "182 1\n",
      "Training: 130 of 184\n",
      "182 1\n",
      "Training: 131 of 184\n",
      "182 1\n",
      "Training: 132 of 184\n",
      "182 1\n",
      "Training: 133 of 184\n",
      "182 1\n",
      "Training: 134 of 184\n",
      "182 1\n",
      "Training: 135 of 184\n",
      "182 1\n",
      "Training: 136 of 184\n",
      "182 1\n",
      "Training: 137 of 184\n",
      "182 1\n",
      "Training: 138 of 184\n",
      "182 1\n",
      "Training: 139 of 184\n",
      "182 1\n",
      "Training: 140 of 184\n",
      "182 1\n",
      "Training: 141 of 184\n",
      "182 1\n",
      "Training: 142 of 184\n",
      "182 1\n",
      "Training: 143 of 184\n",
      "182 1\n",
      "Training: 144 of 184\n",
      "182 1\n",
      "Training: 145 of 184\n",
      "182 1\n",
      "Training: 146 of 184\n",
      "182 1\n",
      "Training: 147 of 184\n",
      "182 1\n",
      "Training: 148 of 184\n",
      "182 1\n",
      "Training: 149 of 184\n",
      "182 1\n",
      "Training: 150 of 184\n",
      "182 1\n",
      "Training: 151 of 184\n",
      "182 1\n",
      "Training: 152 of 184\n",
      "182 1\n",
      "Training: 153 of 184\n",
      "182 1\n",
      "Training: 154 of 184\n",
      "182 1\n",
      "Training: 155 of 184\n",
      "182 1\n",
      "Training: 156 of 184\n",
      "182 1\n",
      "Training: 157 of 184\n",
      "182 1\n",
      "Training: 158 of 184\n",
      "182 1\n",
      "Training: 159 of 184\n",
      "182 1\n",
      "Training: 160 of 184\n",
      "182 1\n",
      "Training: 161 of 184\n",
      "182 1\n",
      "Training: 162 of 184\n",
      "182 1\n",
      "Training: 163 of 184\n",
      "182 1\n",
      "Training: 164 of 184\n",
      "182 1\n",
      "Training: 165 of 184\n",
      "182 1\n",
      "Training: 166 of 184\n",
      "182 1\n",
      "Training: 167 of 184\n",
      "182 1\n",
      "Training: 168 of 184\n",
      "182 1\n",
      "Training: 169 of 184\n",
      "182 1\n",
      "Training: 170 of 184\n",
      "182 1\n",
      "Training: 171 of 184\n",
      "182 1\n",
      "Training: 172 of 184\n",
      "182 1\n",
      "Training: 173 of 184\n",
      "182 1\n",
      "Training: 174 of 184\n",
      "182 1\n",
      "Training: 175 of 184\n",
      "182 1\n",
      "Training: 176 of 184\n",
      "182 1\n",
      "Training: 177 of 184\n",
      "182 1\n",
      "Training: 178 of 184\n",
      "182 1\n",
      "Training: 179 of 184\n",
      "182 1\n",
      "Training: 180 of 184\n",
      "182 1\n",
      "Training: 181 of 184\n",
      "182 1\n",
      "Training: 182 of 184\n",
      "182 1\n",
      "Training: 183 of 184\n",
      "182 1\n",
      "Accuracy: 0.8306010928961749\n",
      "Confusion Matrix:\n",
      "[[80 19]\n",
      " [12 72]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.81      0.84        99\n",
      "           1       0.79      0.86      0.82        84\n",
      "\n",
      "    accuracy                           0.83       183\n",
      "   macro avg       0.83      0.83      0.83       183\n",
      "weighted avg       0.83      0.83      0.83       183\n",
      "\n",
      "ROC AUC: 0.8326118326118326\n",
      "Number of features in the beta-band range: 6\n",
      "       Feature  Mean SHAP Value\n",
      "34   33.203125         0.011243\n",
      "36    35.15625         0.009433\n",
      "31  30.2734375         0.008761\n",
      "37  36.1328125         0.007744\n",
      "35  34.1796875         0.007262\n",
      "18   17.578125         0.006412\n",
      "32       31.25         0.006383\n",
      "30   29.296875         0.006293\n",
      "8       7.8125         0.005698\n",
      "33  32.2265625         0.005376\n",
      "19  18.5546875         0.004898\n",
      "17  16.6015625         0.004820\n",
      "4      3.90625         0.004615\n",
      "27  26.3671875         0.004364\n",
      "16      15.625         0.004327\n",
      "38   37.109375         0.004327\n",
      "9    8.7890625         0.004042\n",
      "7    6.8359375         0.004002\n",
      "41  40.0390625         0.003873\n",
      "3    2.9296875         0.003759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qr/d2wxfs456fsf5lgfshrqbsmw0000gn/T/ipykernel_10585/224699477.py:203: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top_features['FeatureValue'] = top_features['Feature'].apply(\n"
     ]
    }
   ],
   "source": [
    "#LOOCV - suboptimal method to LOPOCV (see below)\n",
    "\n",
    "# Define the optimal channels for each patient\n",
    "def get_hemisphere_channels(patient, channel):\n",
    "    # Extract hemisphere from the channel name\n",
    "    if channel.endswith('_RIGHT'):\n",
    "        hemisphere_channel = 'RIGHT'\n",
    "    elif channel.endswith('_LEFT'):\n",
    "        hemisphere_channel = 'LEFT'\n",
    "    else:\n",
    "        return None  # Return None if it's not a valid channel\n",
    "    \n",
    "    # Return optimal channels based on the patient and hemisphere\n",
    "    if patient == 'DBS036' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_THREE']\n",
    "    if patient == 'DBS009' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_THREE']\n",
    "    if patient == 'DBS014' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS013' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ONE_THREE']\n",
    "    if patient == 'DBS021' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS021' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS005' and hemisphere_channel == 'LEFT':\n",
    "        return ['ONE_THREE']\n",
    "    if patient == 'DBS005' and hemisphere_channel == 'RIGHT':\n",
    "        return ['TWO_THREE']\n",
    "    if patient == 'DBS047' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS047' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS050' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS039' and hemisphere_channel == 'LEFT':\n",
    "        return ['ONE_THREE']\n",
    "    return ['ONE_THREE']\n",
    "\n",
    "def get_optimal_hemisphere(patient): #function returns the optimal hemisphere for a patient, so only PSD from the optimal hemisphere will be used for ML\n",
    "    if patient == 'DBS003':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS005':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS013':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS014':\n",
    "        return 'LEFT'\n",
    "    if patient == 'DBS018':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS021':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS036':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS039':\n",
    "        return 'LEFT'\n",
    "    if patient == 'DBS047':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS050':\n",
    "        return 'LEFT'\n",
    "    return 'RIGHT'\n",
    "\n",
    "PSD_df = pd.read_csv(resultsDir + resultsPSDFilename)\n",
    "PSD_df['RecordingType'] = PSD_df['RecordingType'].replace('CT', 'CT (stim on)')\n",
    "df = pd.read_csv(resultsDir + resultsFilename)\n",
    "df['RecordingType'] = df['RecordingType'].replace('CT', 'CT (stim on)')\n",
    "\n",
    "patList = PSD_df['Patient'].unique() #patList = ['DBS003'], hemisphere = 'RIGHT'\n",
    "\n",
    "PSD_IDpowers = PSD_df.drop(['Implant date','StatusDysStart','Discharge','RecordingStartTime','RecordingType','Channel'], axis=1)\n",
    "\n",
    "aggregated_shap_values = np.zeros(len(freqs))  # Sum SHAP values across folds\n",
    "\n",
    "\n",
    "#USE ALL FREQUENCIES AS FEATURES\n",
    "freqs = [col for col in PSD_df.columns if re.search(r'\\d', col)]\n",
    "freqs_np = np.array(freqs)\n",
    "\n",
    "#Use column 'IntervalType' to determine the label of the recording, and the columns in freqs to determine the features\n",
    "#Create a new dataframe with the features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for index, row in PSD_df.iterrows():\n",
    "    optimal_hemisphere_channels = get_hemisphere_channels(row['Patient'], row['Channel'])\n",
    "    if row['Channel'].endswith('_RIGHT'):\n",
    "        hemisphere = 'RIGHT'\n",
    "        channel = row['Channel'].replace('_RIGHT', '')\n",
    "    elif row['Channel'].endswith('_LEFT'):\n",
    "        hemisphere = 'LEFT'\n",
    "        channel = row['Channel'].replace('_LEFT', '')\n",
    "    #print(row['Patient'], row['Channel'], optimal_hemisphere_channels)\n",
    "    if channel in optimal_hemisphere_channels and hemisphere == get_optimal_hemisphere(row['Patient']): #only use PSDs from the optimal hemisphere and contact\n",
    "        # Now check whether to add to train or test set\n",
    "        X.append(row[freqs].values)\n",
    "        if row['IntervalType'] == 'status':\n",
    "            y.append(1)\n",
    "        elif row['IntervalType'] == 'oneYear':\n",
    "            y.append(0)\n",
    "        else:\n",
    "            print('Error: IntervalType is not SD or oneYear')\n",
    "            break\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(len(y))\n",
    "\n",
    "#Train random forect classifier to predict the label of a new PSD, using leave-one-out cross validation.\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "    #print the progress of the training\n",
    "    print('Training:', len(y_true) + 1, 'of', len(y) + 1)\n",
    "    print(len(X[train_index]), len(X[test_index]))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    clf.fit(np.array(X_train), np.array(y_train))\n",
    "    y_pred.append(clf.predict(X_test))\n",
    "    y_true.append(y_test)\n",
    "\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    if isinstance(shap_values, list):  # Binary classification\n",
    "        # Combine SHAP values across samples\n",
    "        shap_values_combined = np.mean(np.abs(shap_values), axis=0)  # Shape: (n_samples, n_features)\n",
    "        print(shap_values_combined.shape)\n",
    "        # Compute mean absolute SHAP values for each feature\n",
    "        mean_shap_values_per_class = np.mean(shap_values_combined, axis=0)  # Shape: (n_features, 2)\n",
    "\n",
    "        # Take the first column, as both are equal\n",
    "        mean_shap_values = mean_shap_values_per_class[:, 0]  # Shape: (n_features,)\n",
    "    else:\n",
    "        mean_shap_values = np.abs(shap_values).mean(axis=0)  # Shape: (n_features,)\n",
    "\n",
    "\n",
    "    # Accumulate SHAP values\n",
    "    aggregated_shap_values += mean_shap_values[:,0]  # Now properly reduced to (129,)\n",
    "    fold_count += 1\n",
    "\n",
    "# Average SHAP values across folds\n",
    "aggregated_shap_values /= fold_count\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': freqs,\n",
    "    'Mean SHAP Value': aggregated_shap_values\n",
    "}).sort_values(by='Mean SHAP Value', ascending=False)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "y_pred = y_pred.flatten()\n",
    "y_true = y_true.flatten()\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print('ROC AUC:', roc_auc)\n",
    "\n",
    "# Define the beta-band range\n",
    "betaLim = [12.5, 30]\n",
    "\n",
    "# Extract the top 30 features\n",
    "top_features = feature_importance_df.head(20)\n",
    "import re\n",
    "top_features['FeatureValue'] = top_features['Feature'].apply(\n",
    "    lambda x: float(re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", x)[0]) if re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", x) else None\n",
    ")\n",
    "\n",
    "# Count the number of features within the beta-band range\n",
    "num_beta_band_features = top_features['FeatureValue'].apply(\n",
    "    lambda x: betaLim[0] <= x <= betaLim[1] if x is not None else False\n",
    ").sum()\n",
    "\n",
    "print(f\"Number of features in the beta-band range: {num_beta_band_features}\")\n",
    "print(feature_importance_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:92: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:92: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/qr/d2wxfs456fsf5lgfshrqbsmw0000gn/T/ipykernel_10585/3816093064.py:92: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  '''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Patients: ('DBS003',)\n",
      "Test Patients: ('DBS005',)\n",
      "Test Patients: ('DBS013',)\n",
      "Test Patients: ('DBS014',)\n",
      "Test Patients: ('DBS018',)\n",
      "Test Patients: ('DBS021',)\n",
      "Test Patients: ('DBS036',)\n",
      "Test Patients: ('DBS047',)\n",
      "Test Patients: ('DBS050',)\n",
      "Test Patients: ('DBS039',)\n",
      "Accuracy: 0.7486338797814208\n",
      "Confusion Matrix:\n",
      "[[65 34]\n",
      " [12 72]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.66      0.74        99\n",
      "           1       0.68      0.86      0.76        84\n",
      "\n",
      "    accuracy                           0.75       183\n",
      "   macro avg       0.76      0.76      0.75       183\n",
      "weighted avg       0.77      0.75      0.75       183\n",
      "\n",
      "ROC AUC: 0.7568542568542569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nloo = LeaveOneOut()\\ny_pred = []\\ny_true = []\\nfor train_index, test_index in loo.split(X):\\n    #print the progress of the training\\n    print('Training:', len(y_true) + 1, 'of', len(y) + 1)\\n    X_train, X_test = X[train_index], X[test_index]\\n    y_train, y_test = y[train_index], y[test_index]\\n    clf = svm.SVC()\\n    #clf = GMLVQ()\\n    #clf = GrmlvqModel()\\n    #clf = RandomForestClassifier()\\n    #clf = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\\n    clf = XGBClassifier()\\n    clf.fit(np.array(X_train), np.array(y_train))\\n    y_pred.append(clf.predict(X_test))\\n    y_true.append(y_test)\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOPOCV = HAVE SEPARATE TRAINING AND TESTING SET WITH DIFFERENT PATIENTS\n",
    "#Load the PSDs for all recording, and label them as either SD or non-SD\n",
    "\n",
    "# Define the optimal channels for each patient\n",
    "def get_hemisphere_channels(patient, channel):\n",
    "    if channel.endswith('_RIGHT'):\n",
    "        hemisphere_channel = 'RIGHT'\n",
    "    elif channel.endswith('_LEFT'):\n",
    "        hemisphere_channel = 'LEFT'\n",
    "    else:\n",
    "        return None  # Return None if it's not a valid channel\n",
    "    \n",
    "    # Return optimal channels based on the patient and hemisphere\n",
    "    if patient == 'DBS036' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_THREE']\n",
    "    if patient == 'DBS009' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_THREE']\n",
    "    if patient == 'DBS014' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS013' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ONE_THREE']\n",
    "    if patient == 'DBS021' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS021' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS005' and hemisphere_channel == 'LEFT':\n",
    "        return ['ONE_THREE']\n",
    "    if patient == 'DBS005' and hemisphere_channel == 'RIGHT':\n",
    "        return ['TWO_THREE']\n",
    "    if patient == 'DBS047' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS047' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS050' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS039' and hemisphere_channel == 'LEFT':\n",
    "        return ['ONE_THREE']\n",
    "    return ['ONE_THREE']\n",
    "\n",
    "def get_optimal_hemisphere(patient): #function returns the optimal hemisphere for a patient, so only PSD from the optimal hemisphere will be used for ML\n",
    "    if patient == 'DBS003':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS005':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS013':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS014':\n",
    "        return 'LEFT'\n",
    "    if patient == 'DBS018':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS021':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS036':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS039':\n",
    "        return 'LEFT'\n",
    "    if patient == 'DBS047':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS050':\n",
    "        return 'LEFT'\n",
    "    return 'RIGHT'\n",
    "\n",
    "PSD_df = pd.read_csv(resultsDir + resultsPSDFilename)\n",
    "PSD_df['RecordingType'] = PSD_df['RecordingType'].replace('CT', 'CT (stim on)')\n",
    "df = pd.read_csv(resultsDir + resultsFilename)\n",
    "\n",
    "patList = PSD_df['Patient'].unique()\n",
    "\n",
    "test_patList_combos = []\n",
    "no_postSD_pats = ['DBS005', 'DBS014', 'DBS018']  # no non-SD to test foraccuracy of classification of non-SD on\n",
    "test_patList_combos = []\n",
    "\n",
    "for perm in itertools.combinations(patList, 1): #choose every combination of 1 patients as test set, and 9 as training\n",
    "    test_patList_combos.append(perm)\n",
    "\n",
    "    \n",
    "PSD_IDpowers = PSD_df.drop(['Implant date','StatusDysStart','Discharge','RecordingStartTime','RecordingType','Channel'], axis=1)\n",
    "\n",
    "#USE ALL FREQUENCIES AS FEATURES\n",
    "freqs = [col for col in PSD_df.columns if re.search(r'\\d', col)]\n",
    "freqs_np = np.array(freqs)\n",
    "\n",
    "#Use column 'IntervalType' to determine the label of the recording, and the columns in freqs to determine the features\n",
    "#Create a new dataframe with the features and labels\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "for test_patList in test_patList_combos: #iterate through combinations test sets\n",
    "    print('Test Patients:', test_patList)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for index, row in PSD_df.iterrows():\n",
    "        optimal_hemisphere_channels = get_hemisphere_channels(row['Patient'], row['Channel'])\n",
    "        if row['Channel'].endswith('_RIGHT'):\n",
    "            hemisphere = 'RIGHT'\n",
    "            channel = row['Channel'].replace('_RIGHT', '')\n",
    "        elif row['Channel'].endswith('_LEFT'):\n",
    "            hemisphere = 'LEFT'\n",
    "            channel = row['Channel'].replace('_LEFT', '')\n",
    "        if channel in optimal_hemisphere_channels and hemisphere == get_optimal_hemisphere(row['Patient']): #only use PSDs from the optimal hemisphere and contact\n",
    "            # Now check whether to add to train or test set\n",
    "            if row['Patient'] in test_patList:\n",
    "                X_test.append(row[freqs].values)\n",
    "                if row['IntervalType'] == 'status':\n",
    "                    y_test.append(1)\n",
    "                elif row['IntervalType'] == 'oneYear':\n",
    "                    y_test.append(0)\n",
    "                else:\n",
    "                    print('Error: IntervalType is not SD or oneYear')\n",
    "                    break\n",
    "            else:\n",
    "                X_train.append(row[freqs].values)\n",
    "                if row['IntervalType'] == 'status':\n",
    "                    y_train.append(1)\n",
    "                elif row['IntervalType'] == 'oneYear':\n",
    "                    y_train.append(0)\n",
    "                else:\n",
    "                    print('Error: IntervalType is not SD or oneYear')\n",
    "                    break\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    clf = RandomForestClassifier()\n",
    "\n",
    "    clf.fit(np.array(X_train), np.array(y_train))\n",
    "    y_pred.extend(clf.predict(X_test))\n",
    "    y_true.extend(y_test)\n",
    "\n",
    "y_pred = np.array(y_pred)\n",
    "y_true = np.array(y_true)\n",
    "\n",
    "y_pred = y_pred.flatten()\n",
    "y_true = y_true.flatten()\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "roc_auc = roc_auc_score(y_true, y_pred)\n",
    "print('ROC AUC:', roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Patients: ('DBS003',)\n",
      "Test Patients: ('DBS005',)\n",
      "Test Patients: ('DBS013',)\n",
      "Test Patients: ('DBS014',)\n",
      "Test Patients: ('DBS018',)\n",
      "Test Patients: ('DBS021',)\n",
      "Test Patients: ('DBS036',)\n",
      "Test Patients: ('DBS047',)\n",
      "Test Patients: ('DBS050',)\n",
      "Test Patients: ('DBS039',)\n",
      "Accuracy: 0.7431693989071039\n",
      "Confusion Matrix:\n",
      "[[63 36]\n",
      " [11 73]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.64      0.73        99\n",
      "           1       0.67      0.87      0.76        84\n",
      "\n",
      "    accuracy                           0.74       183\n",
      "   macro avg       0.76      0.75      0.74       183\n",
      "weighted avg       0.77      0.74      0.74       183\n",
      "\n",
      "ROC AUC: 0.7527056277056278\n",
      "       Feature  Mean SHAP Value\n",
      "34   33.203125         0.026156\n",
      "31  30.2734375         0.019699\n",
      "36    35.15625         0.019234\n",
      "37  36.1328125         0.016261\n",
      "18   17.578125         0.014353\n",
      "35  34.1796875         0.012451\n",
      "32       31.25         0.011878\n",
      "30   29.296875         0.011073\n",
      "8       7.8125         0.010910\n",
      "19  18.5546875         0.010754\n",
      "16      15.625         0.010453\n",
      "33  32.2265625         0.010329\n",
      "17  16.6015625         0.009738\n",
      "38   37.109375         0.009070\n",
      "4      3.90625         0.008803\n",
      "27  26.3671875         0.008094\n",
      "7    6.8359375         0.007796\n",
      "40     39.0625         0.007473\n",
      "39  38.0859375         0.007464\n",
      "20    19.53125         0.007235\n",
      "9    8.7890625         0.007108\n",
      "41  40.0390625         0.007082\n",
      "21  20.5078125         0.007022\n",
      "42   41.015625         0.006906\n",
      "26   25.390625         0.006782\n",
      "3    2.9296875         0.006767\n",
      "43  41.9921875         0.006536\n",
      "29  28.3203125         0.006404\n",
      "2     1.953125         0.005571\n",
      "22   21.484375         0.005500\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "import shap\n",
    "\n",
    "# Define functions for optimal channels and hemispheres\n",
    "def get_hemisphere_channels(patient, channel):\n",
    "    if channel.endswith('_RIGHT'):\n",
    "        hemisphere_channel = 'RIGHT'\n",
    "    elif channel.endswith('_LEFT'):\n",
    "        hemisphere_channel = 'LEFT'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if patient == 'DBS036' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_THREE']\n",
    "    if patient == 'DBS009' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_THREE']\n",
    "    if patient == 'DBS014' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS013' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ONE_THREE']\n",
    "    if patient == 'DBS021' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS021' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS005' and hemisphere_channel == 'LEFT':\n",
    "        return ['ONE_THREE']\n",
    "    if patient == 'DBS005' and hemisphere_channel == 'RIGHT':\n",
    "        return ['TWO_THREE']\n",
    "    if patient == 'DBS047' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS047' and hemisphere_channel == 'RIGHT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS050' and hemisphere_channel == 'LEFT':\n",
    "        return ['ZERO_TWO']\n",
    "    if patient == 'DBS039' and hemisphere_channel == 'LEFT':\n",
    "        return ['ONE_THREE']\n",
    "    return ['ONE_THREE']\n",
    "\n",
    "def get_optimal_hemisphere(patient):\n",
    "    if patient == 'DBS003':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS005':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS013':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS014':\n",
    "        return 'LEFT'\n",
    "    if patient == 'DBS018':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS021':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS036':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS039':\n",
    "        return 'LEFT'\n",
    "    if patient == 'DBS047':\n",
    "        return 'RIGHT'\n",
    "    if patient == 'DBS050':\n",
    "        return 'LEFT'\n",
    "    return 'RIGHT'\n",
    "\n",
    "# Load data\n",
    "PSD_df = pd.read_csv(resultsDir + resultsPSDFilename)\n",
    "patList = PSD_df['Patient'].unique()\n",
    "\n",
    "# Prepare test patient combinations\n",
    "test_patList_combos = []\n",
    "no_postSD_pats = ['DBS005', 'DBS014', 'DBS018']\n",
    "for perm in itertools.combinations(patList, 1):\n",
    "#    if not any(pat in no_postSD_pats for pat in perm):\n",
    "    test_patList_combos.append(perm)\n",
    "\n",
    "freqs = [col for col in PSD_df.columns if re.search(r'\\d', col)]\n",
    "freqs_np = np.array(freqs)\n",
    "\n",
    "# Initialize variables for results\n",
    "y_pred = []\n",
    "y_true = []\n",
    "aggregated_shap_values = np.zeros(len(freqs))  # Sum SHAP values across folds\n",
    "fold_count = 0\n",
    "\n",
    "# Loop through test patient combinations\n",
    "for test_patList in test_patList_combos:\n",
    "    print('Test Patients:', test_patList)\n",
    "    X_train, y_train, X_test, y_test = [], [], [], []\n",
    "\n",
    "    for index, row in PSD_df.iterrows():\n",
    "        optimal_hemisphere_channels = get_hemisphere_channels(row['Patient'], row['Channel'])\n",
    "        if row['Channel'].endswith('_RIGHT'):\n",
    "            hemisphere = 'RIGHT'\n",
    "            channel = row['Channel'].replace('_RIGHT', '')\n",
    "        elif row['Channel'].endswith('_LEFT'):\n",
    "            hemisphere = 'LEFT'\n",
    "            channel = row['Channel'].replace('_LEFT', '')\n",
    "\n",
    "        if channel in optimal_hemisphere_channels and hemisphere == get_optimal_hemisphere(row['Patient']):\n",
    "            if row['Patient'] in test_patList:\n",
    "                X_test.append(row[freqs].values)\n",
    "                y_test.append(1 if row['IntervalType'] == 'status' else 0)\n",
    "            else:\n",
    "                X_train.append(row[freqs].values)\n",
    "                y_train.append(1 if row['IntervalType'] == 'status' else 0)\n",
    "\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred.extend(clf.predict(X_test))\n",
    "    y_true.extend(y_test)\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "    #set shap_values to be the absolute value of the first row of shap_values\n",
    "\n",
    "    #shap_values = shap_values[:,0]\n",
    "    #print(shap_values.shape)\n",
    "\n",
    "    if isinstance(shap_values, list):  # Binary classification\n",
    "        # Combine SHAP values across samples\n",
    "        shap_values_combined = np.mean(np.abs(shap_values), axis=0)  # Shape: (n_samples, n_features)\n",
    "        print(shap_values_combined.shape)\n",
    "        # Compute mean absolute SHAP values for each feature\n",
    "        mean_shap_values_per_class = np.mean(shap_values_combined, axis=0)  # Shape: (n_features, 2)\n",
    "\n",
    "        # Take the first column, as both are equal\n",
    "        mean_shap_values = mean_shap_values_per_class[:, 0]  # Shape: (n_features,)\n",
    "    else:\n",
    "        mean_shap_values = np.abs(shap_values).mean(axis=0)  # Shape: (n_features,)\n",
    "\n",
    "\n",
    "    # Accumulate SHAP values\n",
    "    aggregated_shap_values += mean_shap_values[:,0]  # Now properly reduced to (129,)\n",
    "    fold_count += 1\n",
    "\n",
    "# Average SHAP values across folds\n",
    "aggregated_shap_values /= fold_count\n",
    "\n",
    "# Create feature importance DataFrame\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': freqs,\n",
    "    'Mean SHAP Value': aggregated_shap_values\n",
    "}).sort_values(by='Mean SHAP Value', ascending=False)\n",
    "\n",
    "# Display results\n",
    "print('Accuracy:', accuracy_score(y_true, y_pred))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('ROC AUC:', roc_auc_score(y_true, y_pred))\n",
    "\n",
    "# Output feature importance\n",
    "print(feature_importance_df.head(30))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
